{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9fd3NU_lA_W",
   "metadata": {
    "id": "f9fd3NU_lA_W"
   },
   "source": [
    "### Import Cleaned E-commerce Dataset\n",
    "The csv file named 'cleaned_ecommerce_dataset.csv' is provided. You may need to use the Pandas method, i.e., `read_csv`, for reading it. After that, please print out its total length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2f4cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1:\n",
      "Training Set Shape: (268, 10)\n",
      "Testing Set Shape: (2417, 10)\n",
      "\n",
      "Case 2:\n",
      "Training Set Shape: (2416, 10)\n",
      "Testing Set Shape: (269, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV file\n",
    "data = pd.read_csv('C:/Users/Wassim/cleaned_ecommerce_dataset.csv') \n",
    "\n",
    "# Since we don't have a specified target, let's use 'rating' as a placeholder\n",
    "features = data.drop('rating', axis=1) \n",
    "target = data['rating']\n",
    "\n",
    "# Split the data\n",
    "\n",
    "# Case 1: 10% Training, 90% Testing \n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(\n",
    "    features,  \n",
    "    target,                \n",
    "    test_size=0.9,          \n",
    "    random_state=42           \n",
    ")\n",
    "\n",
    "print(\"Case 1:\")\n",
    "print(f\"Training Set Shape: {X_train_1.shape}\")\n",
    "print(f\"Testing Set Shape: {X_test_1.shape}\")\n",
    "\n",
    "# Case 2: 90% Training, 10% Testing\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(\n",
    "    features, \n",
    "    target,            \n",
    "    test_size=0.1,           \n",
    "    random_state=42           \n",
    ")\n",
    "\n",
    "print(\"\\nCase 2:\")\n",
    "print(f\"Training Set Shape: {X_train_2.shape}\")\n",
    "print(f\"Testing Set Shape: {X_test_2.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z88FfJc9lA_T",
   "metadata": {
    "id": "Z88FfJc9lA_T"
   },
   "source": [
    "## Analysis of an E-commerce Dataset Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hoq0NwA9lA_V",
   "metadata": {
    "id": "hoq0NwA9lA_V"
   },
   "source": [
    "The goal of the second analysis task is to train linear regression models to predict users' ratings towards items. This involves a standard Data Science workflow: exploring data, building models, making predictions, and evaluating results. In this task, we will explore the impacts of feature selections and different sizes of training/testing data on the model performance. We will use another cleaned combined e-commerce sub-dataset that **is different from** the one in “Analysis of an E-commerce Dataset” task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "PJrb2gtAlA_W",
   "metadata": {
    "id": "PJrb2gtAlA_W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows in the dataset is: 2685\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('C:/Users/Wassim/Downloads/cleaned_ecommerce_dataset.csv')\n",
    "\n",
    "# Get the total length (number of rows)\n",
    "total_length = len(data)\n",
    "\n",
    "# Print the result\n",
    "print(\"The total number of rows in the dataset is:\", total_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqbuU6rglA_X",
   "metadata": {
    "id": "aqbuU6rglA_X"
   },
   "source": [
    "### Explore the Dataset\n",
    "\n",
    "* Use the methods, i.e., `head()` and `info()`, to have a rough picture about the data, e.g., how many columns, and the data types of each column.\n",
    "* As our goal is to predict ratings given other columns, please get the correlations between helpfulness/gender/category/review and rating by using the `corr()` method.\n",
    "\n",
    "  Hints: To get the correlations between different features, you may need to first convert the categorical features (i.e., gender, category and review) into numerial values. For doing this, you may need to import `OrdinalEncoder` from `sklearn.preprocessing` (refer to the useful exmaples [here](https://pbpython.com/categorical-encoding.html))\n",
    "* Please provide ___necessary explanations/analysis___ on the correlations, and figure out which are the ___most___ and ___least___ corrleated features regarding rating. Try to ___discuss___ how the correlation will affect the final prediction results, if we use these features to train a regression model for rating prediction. In what follows, we will conduct experiments to verify your hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "W3PImHiElA_X",
   "metadata": {
    "id": "W3PImHiElA_X",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  timestamp                                           review  \\\n",
      "0    4081      71900                                Not always McCrap   \n",
      "1    4081      72000  I dropped the chalupa even before he told me to   \n",
      "2    4081      72000                     The Wonderful World of Wendy   \n",
      "3    4081     100399                             They actually did it   \n",
      "4    4081     100399                             Hey! Gimme some pie!   \n",
      "\n",
      "                                 item  rating  helpfulness gender  \\\n",
      "0                          McDonald's     4.0          3.0      M   \n",
      "1                           Taco Bell     1.0          4.0      M   \n",
      "2                             Wendy's     5.0          4.0      M   \n",
      "3  South Park: Bigger, Longer & Uncut     5.0          3.0      M   \n",
      "4                        American Pie     3.0          3.0      M   \n",
      "\n",
      "                category  item_id  item_price  user_city  \n",
      "0  Restaurants & Gourmet       41       30.74          4  \n",
      "1  Restaurants & Gourmet       74      108.30          4  \n",
      "2  Restaurants & Gourmet       84       69.00          4  \n",
      "3                 Movies       68      143.11          4  \n",
      "4                 Movies        6      117.89          4  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2685 entries, 0 to 2684\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   userId       2685 non-null   int64  \n",
      " 1   timestamp    2685 non-null   int64  \n",
      " 2   review       2685 non-null   object \n",
      " 3   item         2685 non-null   object \n",
      " 4   rating       2685 non-null   float64\n",
      " 5   helpfulness  2685 non-null   float64\n",
      " 6   gender       2685 non-null   object \n",
      " 7   category     2685 non-null   object \n",
      " 8   item_id      2685 non-null   int64  \n",
      " 9   item_price   2685 non-null   float64\n",
      " 10  user_city    2685 non-null   int64  \n",
      "dtypes: float64(3), int64(4), object(4)\n",
      "memory usage: 230.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# View the first few rows\n",
    "print(data.head())\n",
    "\n",
    "# Summary of the data\n",
    "print(data.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "673e7c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpful     0.938315\n",
      "gender      0.842701\n",
      "category    0.842701\n",
      "review      0.824226\n",
      "rating      1.000000\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Create a sample DataFrame with categorical and numerical features\n",
    "data = pd.DataFrame({\n",
    "    'helpful': [3, 2, 5, 1, 4],\n",
    "    'gender': ['Male', 'Female', 'Male', 'Female', 'Male'],\n",
    "    'category': ['Electronics', 'Clothing', 'Electronics', 'Clothing', 'Electronics'],\n",
    "    'review': ['Positive', 'Negative', 'Positive', 'Negative', 'Neutral'],\n",
    "    'rating': [4, 3, 5, 1, 4]\n",
    "})\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = ['gender', 'category', 'review']  \n",
    "\n",
    "# Create an OrdinalEncoder instance\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# Fit the encoder to the categorical columns and transform  \n",
    "data[categorical_cols] = encoder.fit_transform(data[categorical_cols])\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = data.corr()\n",
    "\n",
    "# Focus on correlations with the 'rating' column\n",
    "rating_correlations = correlations['rating']\n",
    "\n",
    "print(rating_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00192335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpful     0.938315\n",
      "gender      0.842701\n",
      "category    0.842701\n",
      "review      0.824226\n",
      "rating      1.000000\n",
      "Name: rating, dtype: float64\n",
      "Most correlated feature: rating\n",
      "Least correlated feature: review\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlations\n",
    "correlations = data.corr()\n",
    "\n",
    "# Focus on correlations with the 'rating' column\n",
    "rating_correlations = correlations['rating']\n",
    "\n",
    "print(rating_correlations)\n",
    "\n",
    "# Most correlated feature\n",
    "print(\"Most correlated feature:\", rating_correlations.idxmax())\n",
    "\n",
    "# Least correlated feature\n",
    "print(\"Least correlated feature:\", rating_correlations.idxmin())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4myP5igslA_Y",
   "metadata": {
    "id": "4myP5igslA_Y"
   },
   "source": [
    "### Split Training and Testing Data\n",
    "* Machine learning models are trained to help make predictions for the future. Normally, we need to randomly split the dataset into training and testing sets, where we use the training set to train the model, and then leverage the well-trained model to make predictions on the testing set.\n",
    "* To further investigate whether the size of the training/testing data affects the model performance, please random split the data into training and testing sets with different sizes:\n",
    "    * Case 1: training data containing 10% of the entire data;\n",
    "    * Case 2: training data containing 90% of the entire data.\n",
    "* Print the shape of training and testing sets in the two cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8caf8a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1: Training data (10% of entire data)\n",
      "Training set shape: (268, 10)\n",
      "Testing set shape: (2417, 10)\n",
      "\n",
      "Case 2: Training data (90% of entire data)\n",
      "Training set shape: (2416, 10)\n",
      "Testing set shape: (269, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'C:\\Users\\Wassim\\cleaned_ecommerce_dataset.csv')\n",
    "\n",
    "# Assuming 'rating' is the target variable\n",
    "X = df.drop('rating', axis=1)\n",
    "y = df['rating']\n",
    "\n",
    "# Case 1: Training data containing 10% of the entire data\n",
    "X_train_case1, X_test_case1, y_train_case1, y_test_case1 = train_test_split(X, y, test_size=0.90, random_state=42)\n",
    "\n",
    "# Case 2: Training data containing 90% of the entire data\n",
    "X_train_case2, X_test_case2, y_train_case2, y_test_case2 = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "# Print the shapes of training and testing sets for both cases\n",
    "print(\"Case 1: Training data (10% of entire data)\")\n",
    "print(\"Training set shape:\", X_train_case1.shape)\n",
    "print(\"Testing set shape:\", X_test_case1.shape)\n",
    "\n",
    "print(\"\\nCase 2: Training data (90% of entire data)\")\n",
    "print(\"Training set shape:\", X_train_case2.shape)\n",
    "print(\"Testing set shape:\", X_test_case2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a79e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2840d521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1:\n",
      "Training set shape: (268, 10) (268,)\n",
      "Testing set shape: (2417, 10) (2417,)\n",
      "\n",
      "Case 2:\n",
      "Training set shape: (2416, 10) (2416,)\n",
      "Testing set shape: (269, 10) (269,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have your dataset stored in X (features) and y (target variable)\n",
    "\n",
    "# Case 1: Training data containing 10% of the entire data\n",
    "X_train_case1, X_test_case1, y_train_case1, y_test_case1 = train_test_split(X, y, test_size=0.90, random_state=42)\n",
    "\n",
    "# Case 2: Training data containing 90% of the entire data\n",
    "X_train_case2, X_test_case2, y_train_case2, y_test_case2 = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "# Print the shape of training and testing sets for both cases\n",
    "print(\"Case 1:\")\n",
    "print(\"Training set shape:\", X_train_case1.shape, y_train_case1.shape)\n",
    "print(\"Testing set shape:\", X_test_case1.shape, y_test_case1.shape)\n",
    "print(\"\\nCase 2:\")\n",
    "print(\"Training set shape:\", X_train_case2.shape, y_train_case2.shape)\n",
    "print(\"Testing set shape:\", X_test_case2.shape, y_test_case2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349b3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1132d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1f2f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb48064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId           int64\n",
       "timestamp        int64\n",
       "review          object\n",
       "item            object\n",
       "helpfulness    float64\n",
       "gender          object\n",
       "category        object\n",
       "item_id          int64\n",
       "item_price     float64\n",
       "user_city        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9bbcbd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>review</th>\n",
       "      <th>item</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>gender</th>\n",
       "      <th>category</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>user_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>268.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>268</td>\n",
       "      <td>268</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>268</td>\n",
       "      <td>268</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>268.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>268</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Something was deep, but it wasn't the sea</td>\n",
       "      <td>All Advantage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>Movies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142</td>\n",
       "      <td>126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4735.865672</td>\n",
       "      <td>59059.708955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.917910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.660448</td>\n",
       "      <td>81.229813</td>\n",
       "      <td>21.130597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3502.071884</td>\n",
       "      <td>36710.153179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.275015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.421540</td>\n",
       "      <td>42.099419</td>\n",
       "      <td>11.527273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>10100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1393.000000</td>\n",
       "      <td>22100.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4979.500000</td>\n",
       "      <td>53000.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7651.000000</td>\n",
       "      <td>90500.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.250000</td>\n",
       "      <td>126.500000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10710.000000</td>\n",
       "      <td>123199.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              userId      timestamp  \\\n",
       "count     268.000000     268.000000   \n",
       "unique           NaN            NaN   \n",
       "top              NaN            NaN   \n",
       "freq             NaN            NaN   \n",
       "mean     4735.865672   59059.708955   \n",
       "std      3502.071884   36710.153179   \n",
       "min         4.000000   10100.000000   \n",
       "25%      1393.000000   22100.250000   \n",
       "50%      4979.500000   53000.500000   \n",
       "75%      7651.000000   90500.000000   \n",
       "max     10710.000000  123199.000000   \n",
       "\n",
       "                                           review           item  helpfulness  \\\n",
       "count                                         268            268   268.000000   \n",
       "unique                                        268             73          NaN   \n",
       "top     Something was deep, but it wasn't the sea  All Advantage          NaN   \n",
       "freq                                            1             11          NaN   \n",
       "mean                                          NaN            NaN     3.917910   \n",
       "std                                           NaN            NaN     0.275015   \n",
       "min                                           NaN            NaN     3.000000   \n",
       "25%                                           NaN            NaN     4.000000   \n",
       "50%                                           NaN            NaN     4.000000   \n",
       "75%                                           NaN            NaN     4.000000   \n",
       "max                                           NaN            NaN     4.000000   \n",
       "\n",
       "       gender category     item_id  item_price   user_city  \n",
       "count     268      268  268.000000  268.000000  268.000000  \n",
       "unique      2        9         NaN         NaN         NaN  \n",
       "top         F   Movies         NaN         NaN         NaN  \n",
       "freq      142      126         NaN         NaN         NaN  \n",
       "mean      NaN      NaN   45.660448   81.229813   21.130597  \n",
       "std       NaN      NaN   26.421540   42.099419   11.527273  \n",
       "min       NaN      NaN    0.000000   12.000000    0.000000  \n",
       "25%       NaN      NaN   23.500000   49.000000   11.000000  \n",
       "50%       NaN      NaN   46.500000   69.000000   23.000000  \n",
       "75%       NaN      NaN   68.250000  126.500000   31.000000  \n",
       "max       NaN      NaN   88.000000  149.000000   39.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076a1b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "JIDMig9blA_Y",
   "metadata": {
    "id": "JIDMig9blA_Y"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: \"Something was deep, but it wasn't the sea\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10004\\628289911.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Step 3: Model Training and Evaluation (Case 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0my_pred_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mr2_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m                 )\n\u001b[0;32m   1150\u001b[0m             ):\n\u001b[1;32m-> 1151\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[0maccept_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpositive\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coo\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    679\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m         )\n\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1143\u001b[0m         raise ValueError(\n\u001b[0;32m   1144\u001b[0m             \u001b[1;34mf\"{estimator_name} requires y to be passed, but the target y is None\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m         )\n\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1148\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    914\u001b[0m                         )\n\u001b[0;32m    915\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m                 raise ValueError(\n\u001b[0;32m    920\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m                 ) from complex_warning\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2082\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m         if (\n\u001b[0;32m   2086\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2087\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: \"Something was deep, but it wasn't the sea\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "data = pd.read_csv('C:/Users/Wassim/Downloads/cleaned_ecommerce_dataset.csv')\n",
    "\n",
    "# Step 2: Split  Data (Case 1: 10% Training, 90% testing) \n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(\n",
    "    data, data['rating'], test_size=0.9, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Model Training and Evaluation (Case 1)\n",
    "model_1 = LinearRegression()\n",
    "model_1.fit(X_train_1, y_train_1)  \n",
    "y_pred_1 = model_1.predict(X_test_1)\n",
    "\n",
    "r2_1 = r2_score(y_test_1, y_pred_1)\n",
    "mse_1 = mean_squared_error(y_test_1, y_pred_1)\n",
    "\n",
    "print(\"Case 1 Results (10% Training Data):\")\n",
    "print('R-squared:', r2_1)\n",
    "print('Mean Squared Error:', mse_1)\n",
    "\n",
    "# Step 4: Split  Data (Case 2: 90% Training, 10% Testing) \n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(\n",
    "    data, data['rating'], test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Step 5: Model Training and Evaluation (Case 2)\n",
    "model_2 = LinearRegression()\n",
    "model_2.fit(X_train_2, y_train_2) \n",
    "y_pred_2 = model_2.predict(X_test_2)\n",
    "\n",
    "r2_2 = r2_score(y_test_2, y_pred_2)\n",
    "mse_2 = mean_squared_error(y_test_2, y_pred_2)\n",
    "\n",
    "print(\"\\nCase 2 Results (90% Training Data):\")\n",
    "print('R-squared:', r2_2)\n",
    "print('Mean Squared Error:', mse_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DjSsgT0BlA_Y",
   "metadata": {
    "id": "DjSsgT0BlA_Y"
   },
   "source": [
    "### Train Linear Regression Models with Feature Selection under Cases 1 & 2\n",
    "* When training a machine learning model for prediction, we may need to select the most important/correlated input features for more accurate results.\n",
    "* To investigate whether feature selection affects the model performance, please select two most correlated features and two least correlated features from helpfulness/gender/category/review regarding rating, respectively.\n",
    "* Train four linear regression models by following the conditions:\n",
    "    - (model-a) using the training/testing data in case 1 with two most correlated input features\n",
    "    - (model-b) using the training/testing data in case 1 with two least correlated input features\n",
    "    - (model-c) using the training/testing data in case 2 with two most correlated input features\n",
    "    - (model-d) using the training/testing data in case 2 with two least correlated input features\n",
    "* By doing this, we can verify the impacts of the size of traing/testing data on the model performance via comparing model-a and model-c (or model-b and model-d); meanwhile the impacts of feature selection can be validated via comparing model-a and model-b (or model-c and model-d).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77d2b129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Errors:\n",
      "Model A (Case 1, all features): 2.3686227315749044\n",
      "Model B (Case 1, gender and category only): 2.3686227315749044\n",
      "Model C (Case 2, all features): 12.171902103846056\n",
      "Model D (Case 2, gender and category only): 12.171902103846056\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Assuming 'rating' is the target variable\n",
    "X = df.drop('rating', axis=1)\n",
    "y = df['rating']\n",
    "\n",
    "# Split the data into training and testing sets for both cases\n",
    "X_train_case1, X_test_case1, y_train_case1, y_test_case1 = train_test_split(X, y, test_size=0.90, random_state=42)\n",
    "X_train_case2, X_test_case2, y_train_case2, y_test_case2 = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "# Define preprocessing steps for categorical, text, and numerical features\n",
    "categorical_features = ['gender', 'category']\n",
    "text_features = ['review']\n",
    "numerical_features = ['helpfulness']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_features),\n",
    "        ('text', CountVectorizer(), 'review'),\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ])\n",
    "\n",
    "# Define a function to train linear regression models\n",
    "def train_linear_regression_model(X_train, X_test, y_train, y_test, features):\n",
    "    # Preprocess the data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Initialize and train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_processed, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return model, mse\n",
    "\n",
    "# Train models for each case and feature selection\n",
    "model_a_case1, mse_a_case1 = train_linear_regression_model(X_train_case1, X_test_case1, y_train_case1, y_test_case1, X_train_case1.columns)\n",
    "model_b_case1, mse_b_case1 = train_linear_regression_model(X_train_case1, X_test_case1, y_train_case1, y_test_case1, ['gender', 'category'])\n",
    "model_c_case2, mse_c_case2 = train_linear_regression_model(X_train_case2, X_test_case2, y_train_case2, y_test_case2, X_train_case2.columns)\n",
    "model_d_case2, mse_d_case2 = train_linear_regression_model(X_train_case2, X_test_case2, y_train_case2, y_test_case2, ['gender', 'category'])\n",
    "\n",
    "# Print mean squared errors for each model\n",
    "print(\"Mean Squared Errors:\")\n",
    "print(\"Model A (Case 1, all features):\", mse_a_case1)\n",
    "print(\"Model B (Case 1, gender and category only):\", mse_b_case1)\n",
    "print(\"Model C (Case 2, all features):\", mse_c_case2)\n",
    "print(\"Model D (Case 2, gender and category only):\", mse_d_case2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae10979b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gender'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:447\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m--> 447\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m all_columns\u001b[38;5;241m.\u001b[39mget_loc(col)\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gender'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, mse\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Train models for each case and feature selection\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m model_a_case1, mse_a_case1 \u001b[38;5;241m=\u001b[39m train_linear_regression_model(X_train_case1, X_test_case1, y_train_case1, y_test_case1, features_case1_most_correlated)\n\u001b[0;32m     46\u001b[0m model_b_case1, mse_b_case1 \u001b[38;5;241m=\u001b[39m train_linear_regression_model(X_train_case1, X_test_case1, y_train_case1, y_test_case1, features_case1_least_correlated)\n\u001b[0;32m     47\u001b[0m model_c_case2, mse_c_case2 \u001b[38;5;241m=\u001b[39m train_linear_regression_model(X_train_case2, X_test_case2, y_train_case2, y_test_case2, features_case2_most_correlated)\n",
      "Cell \u001b[1;32mIn[12], line 34\u001b[0m, in \u001b[0;36mtrain_linear_regression_model\u001b[1;34m(X_train, X_test, y_train, y_test, features)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_linear_regression_model\u001b[39m(X_train, X_test, y_train, y_test, features):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Initialize and train the linear regression model\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     model \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     31\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[0;32m     32\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m'\u001b[39m, LinearRegression())\n\u001b[0;32m     33\u001b[0m     ])\n\u001b[1;32m---> 34\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train[features], y_train)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test[features])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:416\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    415\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 416\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    368\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    371\u001b[0m     cloned_transformer,\n\u001b[0;32m    372\u001b[0m     X,\n\u001b[0;32m    373\u001b[0m     y,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    375\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    376\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:740\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[1;32m--> 740\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    743\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:448\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    446\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    447\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 448\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m _get_column_indices(X, columns)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:455\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    452\u001b[0m             column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 455\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA given column is not a column of the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the subsets of features for each case\n",
    "features_case1_most_correlated = ['helpfulness', 'review']\n",
    "features_case1_least_correlated = ['gender', 'category']\n",
    "features_case2_most_correlated = ['helpfulness', 'review']\n",
    "features_case2_least_correlated = ['gender', 'category']\n",
    "\n",
    "# Preprocess categorical and text features separately\n",
    "categorical_features = ['gender', 'category']\n",
    "text_features = ['review']\n",
    "numerical_features = ['helpfulness']\n",
    "\n",
    "# Define preprocessing steps for categorical, text, and numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_features),\n",
    "        ('text', CountVectorizer(), 'review'),\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ])\n",
    "\n",
    "# Define a function to train linear regression models\n",
    "def train_linear_regression_model(X_train, X_test, y_train, y_test, features):\n",
    "    # Initialize and train the linear regression model\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "    model.fit(X_train[features], y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test[features])\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return model, mse\n",
    "\n",
    "# Train models for each case and feature selection\n",
    "model_a_case1, mse_a_case1 = train_linear_regression_model(X_train_case1, X_test_case1, y_train_case1, y_test_case1, features_case1_most_correlated)\n",
    "model_b_case1, mse_b_case1 = train_linear_regression_model(X_train_case1, X_test_case1, y_train_case1, y_test_case1, features_case1_least_correlated)\n",
    "model_c_case2, mse_c_case2 = train_linear_regression_model(X_train_case2, X_test_case2, y_train_case2, y_test_case2, features_case2_most_correlated)\n",
    "model_d_case2, mse_d_case2 = train_linear_regression_model(X_train_case2, X_test_case2, y_train_case2, y_test_case2, features_case2_least_correlated)\n",
    "\n",
    "# Print mean squared errors for each model\n",
    "print(\"Mean Squared Errors:\")\n",
    "print(\"Model A (Case 1, most correlated features):\", mse_a_case1)\n",
    "print(\"Model B (Case 1, least correlated features):\", mse_b_case1)\n",
    "prin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c06a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate correlations between features and target variable\n",
    "correlations = df.corr()['rating'].drop('rating')\n",
    "\n",
    "# Get two most correlated features\n",
    "features_most_correlated = correlations.abs().nlargest(2).index.tolist()\n",
    "\n",
    "# Get two least correlated features\n",
    "features_least_correlated = correlations.abs().nsmallest(2).index.tolist()\n",
    "\n",
    "# Select features for each case\n",
    "features_case1_most_correlated = features_most_correlated\n",
    "features_case1_least_correlated = features_least_correlated\n",
    "features_case2_most_correlated = features_most_correlated\n",
    "features_case2_least_correlated = features_least_correlated\n",
    "\n",
    "# Define a function to train linear regression models\n",
    "def train_linear_regression_model(X_train, X_test, y_train, y_test, features):\n",
    "    # Select subset of features\n",
    "    X_train_subset = X_train[features]\n",
    "    X_test_subset = X_test[features]\n",
    "\n",
    "    # Initialize and train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_subset, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_subset)\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return model, mse\n",
    "\n",
    "# Train models for each case and feature selection\n",
    "model_a_case1, mse_a_case1 = train_linear_regression_model(X_train_case1, X_test_case1, y_train_case1, y_test_case1, features_case1_most_correlated)\n",
    "model_b_case1, mse_b_case1 = train_linear_regression_model(X_train_case1, X_test_case1, y_train_case1, y_test_case1, features_case1_least_correlated)\n",
    "model_c_case2, mse_c_case2 = train_linear_regression_model(X_train_case2, X_test_case2, y_train_case2, y_test_case2, features_case2_most_correlated)\n",
    "model_d_case2, mse_d_case2 = train_linear_regression_model(X_train_case2, X_test_case2, y_train_case2, y_test_case2, features_case2_least_correlated)\n",
    "\n",
    "# Print mean squared errors for each model\n",
    "print(\"Mean Squared Errors:\")\n",
    "print(\"Model A (Case 1, most correlated features):\", mse_a_case1)\n",
    "print(\"Model B (Case 1, least correlated features):\", mse_b_case1)\n",
    "print(\"Model C (Case 2, most correlated features):\", mse_c_case2)\n",
    "print(\"Model D (Case 2, least correlated features):\", mse_d_case2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b1d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the subsets of features for each case\n",
    "features_case1_most_correlated = ['helpfulness', 'review']\n",
    "features_case1_least_correlated = ['gender', 'category']\n",
    "features_case2_most_correlated = ['helpfulness', 'review']\n",
    "features_case2_least_correlated = ['gender', 'category']\n",
    "\n",
    "# Define preprocessing steps for categorical and text features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), ['gender', 'category']),\n",
    "        ('text', CountVectorizer(), 'review')\n",
    "    ])\n",
    "\n",
    "# Train linear regression models\n",
    "model_a_case1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "]).fit(X_train_case1, y_train_case1)\n",
    "\n",
    "model_b_case1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "]).fit(X_train_case1, y_train_case1)\n",
    "\n",
    "model_c_case2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "]).fit(X_train_case2, y_train_case2)\n",
    "\n",
    "model_d_case2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "]).fit(X_train_case2, y_train_case2)\n",
    "\n",
    "# Make predictions and evaluate the models\n",
    "mse_a_case1 = mean_squared_error(y_test_case1, model_a_case1.predict(X_test_case1))\n",
    "mse_b_case1 = mean_squared_error(y_test_case1, model_b_case1.predict(X_test_case1))\n",
    "mse_c_case2 = mean_squared_error(y_test_case2, model_c_case2.predict(X_test_case2))\n",
    "mse_d_case2 = mean_squared_error(y_test_case2, model_d_case2.predict(X_test_case2))\n",
    "\n",
    "# Print mean squared errors for each model\n",
    "print(\"Mean Squared Errors:\")\n",
    "print(\"Model A (Case 1, most correlated features):\", mse_a_case1)\n",
    "print(\"Model B (Case 1, least correlated features):\", mse_b_case1)\n",
    "print(\"Model C (Case 2, most correlated features):\", mse_c_case2)\n",
    "print(\"Model D (Case 2, least correlated features):\", mse_d_case2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the subsets of features for each case\n",
    "features_case1_most_correlated = ['helpfulness', 'review']\n",
    "features_case1_least_correlated = ['gender', 'category']\n",
    "features_case2_most_correlated = ['helpfulness', 'review']\n",
    "features_case2_least_correlated = ['gender', 'category']\n",
    "\n",
    "# Preprocess categorical and numerical features separately\n",
    "categorical_features = ['gender', 'category', 'review']\n",
    "numerical_features = ['helpfulness']\n",
    "\n",
    "# Define preprocessing steps for categorical and numerical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Train linear regression models\n",
    "model_a_case1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "]).fit(X_train_case1, y_train_case1)\n",
    "\n",
    "model_b_case1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "]).fit(X_train_case1, y_train_case1)\n",
    "\n",
    "model_c_case2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "]).fit(X_train_case2, y_train_case2)\n",
    "\n",
    "model_d_case2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "]).fit(X_train_case2, y_train_case2)\n",
    "\n",
    "# Make predictions and evaluate the models\n",
    "mse_a_case1 = mean_squared_error(y_test_case1, model_a_case1.predict(X_test_case1))\n",
    "mse_b_case1 = mean_squared_error(y_test_case1, model_b_case1.predict(X_test_case1))\n",
    "mse_c_case2 = mean_squared_error(y_test_case2, model_c_case2.predict(X_test_case2))\n",
    "mse_d_case2 = mean_squared_error(y_test_case2, model_d_case2.predict(X_test_case2))\n",
    "\n",
    "# Print mean squared errors for each model\n",
    "print(\"Mean Squared Errors:\")\n",
    "print(\"Model A (Case 1, most correlated features):\", mse_a_case1)\n",
    "print(\"Model B (Case 1, least correlated features):\", mse_b_case1)\n",
    "print(\"Model C (Case 2, most correlated features):\", mse_c_case2)\n",
    "print(\"Model D (Case 2, least correlated features):\", mse_d_case2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4687710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define the subsets of features for each case\n",
    "features_case1_most_correlated = ['helpfulness', 'review']\n",
    "features_case1_least_correlated = ['gender', 'category']\n",
    "features_case2_most_correlated = ['helpfulness', 'review']\n",
    "features_case2_least_correlated = ['gender', 'category']\n",
    "\n",
    "# Preprocess categorical features\n",
    "ct = ColumnTransformer([('encoder', OneHotEncoder(), ['gender', 'category', 'review'])], remainder='passthrough')\n",
    "X_train_case1_processed = ct.fit_transform(X_train_case1)\n",
    "X_test_case1_processed = ct.transform(X_test_case1)\n",
    "X_train_case2_processed = ct.fit_transform(X_train_case2)\n",
    "X_test_case2_processed = ct.transform(X_test_case2)\n",
    "\n",
    "# Train linear regression models\n",
    "model_a_case1 = LinearRegression().fit(X_train_case1_processed, y_train_case1)\n",
    "model_b_case1 = LinearRegression().fit(X_train_case1_processed, y_train_case1)\n",
    "model_c_case2 = LinearRegression().fit(X_train_case2_processed, y_train_case2)\n",
    "model_d_case2 = LinearRegression().fit(X_train_case2_processed, y_train_case2)\n",
    "\n",
    "# Make predictions and evaluate the models\n",
    "mse_a_case1 = mean_squared_error(y_test_case1, model_a_case1.predict(X_test_case1_processed))\n",
    "mse_b_case1 = mean_squared_error(y_test_case1, model_b_case1.predict(X_test_case1_processed))\n",
    "mse_c_case2 = mean_squared_error(y_test_case2, model_c_case2.predict(X_test_case2_processed))\n",
    "mse_d_case2 = mean_squared_error(y_test_case2, model_d_case2.predict(X_test_case2_processed))\n",
    "\n",
    "# Print mean squared errors for each model\n",
    "print(\"Mean Squared Errors:\")\n",
    "print(\"Model A (Case 1, most correlated features):\", mse_a_case1)\n",
    "print(\"Model B (Case 1, least correlated features):\", mse_b_case1)\n",
    "print(\"Model C (Case 2, most correlated features):\", mse_c_case2)\n",
    "print(\"Model D (Case 2, least correlated features):\", mse_d_case2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f546b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the subsets of features for each case\n",
    "features_case1_most_correlated = ['helpfulness', 'review']\n",
    "features_case1_least_correlated = ['gender', 'category']\n",
    "features_case2_most_correlated = ['helpfulness', 'review']\n",
    "features_case2_least_correlated = ['gender', 'category']\n",
    "\n",
    "# Train linear regression models\n",
    "def train_linear_regression_model(X_train, X_test, y_train, y_test, features):\n",
    "    # Select the subset of features\n",
    "    X_train_subset = X_train[features]\n",
    "    X_test_subset = X_test[features]\n",
    "    \n",
    "    # Initialize and train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_subset, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_subset)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return model, mse\n",
    "\n",
    "# Train models for each case and feature selection\n",
    "model_a_case1, mse_a_case1 = train_linear_regression_model(X_train_case1, X_test_case1, y_train_case1, y_test_case1, features_case1_most_correlated)\n",
    "model_b_case1, mse_b_case1 = train_linear_regression_model(X_train_case1, X_test_case1, y_train_case1, y_test_case1, features_case1_least_correlated)\n",
    "model_c_case2, mse_c_case2 = train_linear_regression_model(X_train_case2, X_test_case2, y_train_case2, y_test_case2, features_case2_most_correlated)\n",
    "model_d_case2, mse_d_case2 = train_linear_regression_model(X_train_case2, X_test_case2, y_train_case2, y_test_case2, features_case2_least_correlated)\n",
    "\n",
    "# Print mean squared errors for each model\n",
    "print(\"Mean Squared Errors:\")\n",
    "print(\"Model A (Case 1, most correlated features):\", mse_a_case1)\n",
    "print(\"Model B (Case 1, least correlated features):\", mse_b_case1)\n",
    "print(\"Model C (Case 2, most correlated features):\", mse_c_case2)\n",
    "print(\"Model D (Case 2, least correlated features):\", mse_d_case2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1dbdfbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10004\\823133387.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Load dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Wassim/cleaned_ecommerce_dataset.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Select features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mmost_correlated_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleast_correlated_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Split data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mcase1_train_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase1_test_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase1_train_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase1_test_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase2_train_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase2_test_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase2_train_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase2_test_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         ):\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'features'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Function to select two most correlated features and two least correlated features\n",
    "def select_features(features):\n",
    "    # Implement feature selection logic here\n",
    "    return most_correlated_features, least_correlated_features\n",
    "\n",
    "# Function to split the dataset into training and testing data for case 1 and case 2\n",
    "def split_data(dataset):\n",
    "    # Implement data splitting logic for case 1 and case 2 here\n",
    "    return case1_train_X, case1_test_X, case1_train_y, case1_test_y, case2_train_X, case2_test_X, case2_train_y, case2_test_y\n",
    "\n",
    "# Function to train linear regression model\n",
    "def train_model(train_X, train_y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_X, train_y)\n",
    "    return model\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, test_X, test_y):\n",
    "    # Implement evaluation logic here\n",
    "    return model_performance\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv('C:/Users/Wassim/cleaned_ecommerce_dataset.csv')\n",
    "\n",
    "# Select features\n",
    "most_correlated_features, least_correlated_features = select_features(dataset.features)\n",
    "\n",
    "# Split data\n",
    "case1_train_X, case1_test_X, case1_train_y, case1_test_y, case2_train_X, case2_test_X, case2_train_y, case2_test_y = split_data(dataset)\n",
    "\n",
    "# Train models\n",
    "model_a = train_model(case1_train_X[most_correlated_features], case1_train_y)\n",
    "model_b = train_model(case1_train_X[least_correlated_features], case1_train_y)\n",
    "model_c = train_model(case2_train_X[most_correlated_features], case2_train_y)\n",
    "model_d = train_model(case2_train_X[least_correlated_features], case2_train_y)\n",
    "\n",
    "# Evaluate models\n",
    "performance_a = evaluate_model(model_a, case1_test_X[most_correlated_features], case1_test_y)\n",
    "performance_b = evaluate_model(model_b, case1_test_X[least_correlated_features], case1_test_y)\n",
    "performance_c = evaluate_model(model_c, case2_test_X[most_correlated_features], case2_test_y)\n",
    "performance_d = evaluate_model(model_d, case2_test_X[least_correlated_features], case2_test_y)\n",
    "\n",
    "# Print model performances\n",
    "print(\"Model-a Performance:\", performance_a)\n",
    "print(\"Model-b Performance:\", performance_b)\n",
    "print(\"Model-c Performance:\", performance_c)\n",
    "print(\"Model-d Performance:\", performance_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d1b1b20",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Not always McCrap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Wassim/cleaned_ecommerce_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate correlations\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Identify most and least correlated features (adjust column names)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m most_corr_features \u001b[38;5;241m=\u001b[39m corr_matrix[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnlargest(\u001b[38;5;241m3\u001b[39m)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist() \n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10704\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[1;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[0;32m  10702\u001b[0m cols \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m  10703\u001b[0m idx \u001b[38;5;241m=\u001b[39m cols\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m> 10704\u001b[0m mat \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, na_value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m  10706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  10707\u001b[0m     correl \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mnancorr(mat, minp\u001b[38;5;241m=\u001b[39mmin_periods)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1889\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1888\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[1;32m-> 1889\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mas_array(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, na_value\u001b[38;5;241m=\u001b[39mna_value)\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[0;32m   1891\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(result, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1656\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1654\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1656\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interleave(dtype\u001b[38;5;241m=\u001b[39mdtype, na_value\u001b[38;5;241m=\u001b[39mna_value)\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1715\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1713\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1714\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[1;32m-> 1715\u001b[0m     result[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m   1716\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Not always McCrap'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "df.dropna()\n",
    "\n",
    "# Load your dataset (r)\n",
    "df = pd.read_csv('C:/Users/Wassim/cleaned_ecommerce_dataset.csv')\n",
    "\n",
    "# Calculate correlations\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Identify most and least correlated features (adjust column names)\n",
    "most_corr_features = corr_matrix['rating'].nlargest(3)[1:3].index.tolist() \n",
    "least_corr_features = corr_matrix['rating'].nsmallest(3)[:-1].index.tolist()\n",
    "\n",
    "# Case 1 Data\n",
    "data_case_1 = df[['rating'] + most_corr_features]\n",
    "\n",
    "# Case 2 Data\n",
    "data_case_2 = df[['rating'] + least_corr_features]\n",
    "\n",
    "# Function for model training and evaluation\n",
    "def train_and_evaluate(data, model_name):\n",
    "    X = data.drop('rating', axis=1)\n",
    "    y = data['rating']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)  \n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f'Model: {model_name}')\n",
    "    print('R-squared:', r2_score(y_test, y_pred))\n",
    "    print('Mean Squared Error:', mean_squared_error(y_test, y_pred))\n",
    "    print('-------------------')\n",
    "\n",
    "# Train the models\n",
    "train_and_evaluate(data_case_1, 'model-a')\n",
    "train_and_evaluate(data_case_1, 'model-b') \n",
    "train_and_evaluate(data_case_2, 'model-c') \n",
    "train_and_evaluate(data_case_2, 'model-d') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# ---------------------- Step 1: Data Preprocessing ----------------------\n",
    "# Load your dataset (replace plac\n",
    "df = pd.read_csv('C:/Users/Wassim/cleaned_ecommerce_dataset.csv'your_data.csv')\n",
    "\n",
    "# Handle missing values (adjust as needed)\n",
    "data.dropna(inplace=True)  # Example: remove rows with missing values\n",
    "\n",
    "# Encode categorical features (if necessary)\n",
    "data = pd.get_dummies(data, columns=['gender', 'category']) \n",
    "\n",
    "# ---------------------- Step 2: Correlation Analysis ----------------------\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "most_corr_features = corr_matrix['rating'].nlargest(3)[1:].index.tolist()  # Top 2 (+ rating)\n",
    "least_corr_features = corr_matrix['rating'].nsmallest(3)[1:].index.tolist()  # Bottom 2 (+ rating) \n",
    "\n",
    "# ---------------------- Step 3: Feature Selection ----------------------\n",
    "datasets = {\n",
    "    'A': data[['rating'] + most_corr_features],\n",
    "    'B': data[['rating'] + least_corr_features],\n",
    "    'C': data[['rating'] + most_corr_features],  # Same as A in this example, adjust for Case 2\n",
    "    'D': data[['rating'] + least_corr_features]  # Same as B in this example, adjust for Case 2\n",
    "}\n",
    "\n",
    "# ---------------------- Step 4: Model Building ----------------------\n",
    "models = {}\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    # Define your Case 1 and Case 2 test sizes here\n",
    "    test_size_case_1 = 0.2  \n",
    "    test_size_case_2 = 0.3  # (Adjust as needed)\n",
    "\n",
    "    if dataset_name in ['A', 'B']:\n",
    "        test_size = test_size_case_1\n",
    "    else:\n",
    "        test_size = test_size_case_2\n",
    "\n",
    "    X = dataset.drop('rating', axis=1)\n",
    "    y = dataset['rating']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    models[dataset_name] = {\n",
    "        'model': model,\n",
    "        'r2': r2_score(y_test, y_pred),\n",
    "        'mse': mean_squared_error(y_test, y_pred),\n",
    "        'mae': mean_absolute_error(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# ---------------------- Step 5: Evaluation ----------------------\n",
    "print(\"Model Performance:\")\n",
    "for dataset_name, metrics in models.items():\n",
    "    print(f\"\\nModel {dataset_name}:\")\n",
    "    print(f\"  R-squared: {metrics['r2']}\")\n",
    "    print(f\"  MSE: {metrics['mse']}\")\n",
    "    print(f\"  MAE: {metrics['mae']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "380bc45c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Not always McCrap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_sentiment)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Correlation Analysis\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[0;32m     23\u001b[0m most_corr_features \u001b[38;5;241m=\u001b[39m corr_matrix[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnlargest(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     24\u001b[0m least_corr_features \u001b[38;5;241m=\u001b[39m corr_matrix[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnsmallest(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10704\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[1;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[0;32m  10702\u001b[0m cols \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m  10703\u001b[0m idx \u001b[38;5;241m=\u001b[39m cols\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m> 10704\u001b[0m mat \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, na_value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m  10706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  10707\u001b[0m     correl \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mnancorr(mat, minp\u001b[38;5;241m=\u001b[39mmin_periods)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1889\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1888\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[1;32m-> 1889\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mas_array(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, na_value\u001b[38;5;241m=\u001b[39mna_value)\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[0;32m   1891\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(result, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1656\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1654\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1656\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interleave(dtype\u001b[38;5;241m=\u001b[39mdtype, na_value\u001b[38;5;241m=\u001b[39mna_value)\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1715\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1713\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1714\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[1;32m-> 1715\u001b[0m     result[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m   1716\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Not always McCrap'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"C:/Users/Wassim/cleaned_ecommerce_dataset.csv\")\n",
    "\n",
    "# Preprocessing \n",
    "df = df.dropna() \n",
    "\n",
    "# Sentiment Analysis\n",
    "def get_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(text)['compound']  \n",
    "\n",
    "df['sentiment'] = df['review'].apply(get_sentiment)\n",
    "\n",
    "# Correlation Analysis\n",
    "corr_matrix = df.corr()\n",
    "most_corr_features = corr_matrix['rating'].nlargest(3).index[1:]\n",
    "least_corr_features = corr_matrix['rating'].nsmallest(3).index[1:]\n",
    "\n",
    "# Function for model training and evaluation\n",
    "def train_and_evaluate(features, X_train, X_test, y_train, y_test):\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    return r2, mse\n",
    "\n",
    "# ------ CASE 1: 80/20 SPLIT --------\n",
    "X = df[['helpfulness', 'gender', 'category', 'sentiment']]  # Use 'sentiment'\n",
    "y = df['rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model-a: Most correlated \n",
    "r2_a, mse_a = train_and_evaluate(most_corr_features, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Model-b: Least correlated\n",
    "r2_b, mse_b = train_and_evaluate(least_corr_features, X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "# ------ CASE 2: 90/10 SPLIT --------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) \n",
    "\n",
    "# Model-c: Most correlated \n",
    "r2_c, mse_c = train_and_evaluate(most_corr_features, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Model-d: Least correlated\n",
    "r2_d, mse_d = train_and_evaluate(least_corr_features, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# -------- Results --------\n",
    "print(\"CASE 1:\")\n",
    "print(\"Model-a (Most Correlated): R2 =\", r2_a, \"MSE =\", mse_a)\n",
    "print(\"Model-b (Least Correlated): R2 =\", r2_b, \"MSE =\", mse_b)\n",
    "\n",
    "print(\"CASE 2:\")\n",
    "print(\"Model-c (Most Correlated): R2 =\", r2_c, \"MSE =\", mse_c)\n",
    "print(\"Model-d (Least Correlated): R2 =\", r2_d, \"MSE =\", mse_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # A crucial corpus for TextBlob\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from textblob import TextBlob  # Example sentiment analysis library\n",
    "\n",
    "# ---------------------- Step 1: Data Preprocessing ----------------------\n",
    "# Load your dataset (replace placeholder)\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Handle missing values (adjust as needed)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical features (if necessary)\n",
    "data = pd.get_dummies(data, columns=['gender', 'category']) \n",
    "\n",
    "# ---------------------- Step 2: Sentiment Analysis  ----------------------\n",
    "def get_sentiment(review):\n",
    "    \"\"\"Calculates sentiment polarity using TextBlob\"\"\"\n",
    "    analysis = TextBlob(review)\n",
    "    return analysis.sentiment.polarity  # Polarity ranges from -1 (negative) to 1 (positive) \n",
    "\n",
    "# Apply Sentiment Analysis\n",
    "data['sentiment'] = data['review'].apply(get_sentiment)\n",
    "\n",
    "# ---------------------- Step 3: Correlation Analysis ----------------------\n",
    "# Select appropriate features (adjust as needed)\n",
    "features = ['rating', 'helpfulness', 'sentiment']  \n",
    "corr_matrix = data[features].corr()\n",
    "\n",
    "# ... (Rest of your code to select features, build models, and evaluate) ...  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\/Users/Wassim/cleaned_ecommerce_dataset.csv\")\n",
    "df.head(10)  # Show the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"C:/Users/Wassim/cleaned_ecommerce_dataset.csv\")\n",
    "\n",
    "# Sentiment Analysis Function\n",
    "def get_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(text)['compound']  \n",
    "\n",
    "# Add a sentiment column\n",
    "df['sentiment'] = df['review'].apply(get_sentiment)\n",
    "\n",
    "# Feature Selection - Update your feature set\n",
    "X = df[['helpfulness', 'gender', 'category', 'sentiment']] \n",
    "y = df['rating']\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load your CSV dataset\n",
    "df = pd.read_csv(\"C:/Users/Wassim/cleaned_ecommerce_dataset.csv\")\n",
    "\n",
    "# Preprocessing (Illustrative - adapt as needed)\n",
    "df = df.dropna()  \n",
    "df = df[(np.abs(df['rating'] - df['rating'].mean()) <= (3 * df['rating'].std()))] \n",
    "\n",
    "# Correlation Analysis\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "most_correlated_features = corr_matrix['rating'].nlargest(3).index[1:]\n",
    "least_correlated_features = corr_matrix['rating'].nsmallest(3).index[1:]\n",
    "\n",
    "print(\"Most Correlated Features:\", most_correlated_features)\n",
    "print(\"Least Correlated Features:\", least_correlated_features)\n",
    "\n",
    "# Feature Selection and Model Building\n",
    "def train_and_evaluate(features, X_train, X_test, y_train, y_test):\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "    return r2, mse\n",
    "\n",
    "# Case 1: 80/20 split\n",
    "X = df[['helpfulness', 'gender', 'category', 'review']]  # Adjust columns if needed\n",
    "y = df['rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model-a: Most correlated features\n",
    "r2_a, mse_a = train_and_evaluate(most_correlated_features, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Model-b: Least correlated features\n",
    "r2_b, mse_b = train_and_evaluate(least_correlated_features, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Case 2: 90/10 split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) \n",
    "\n",
    "# Model-c \n",
    "r2_c, mse_c = train_and_evaluate(most_correlated_features, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Model-d\n",
    "r2_d, mse_d = train_and_evaluate(least_correlated_features, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Results and Analysis\n",
    "print(\"-------- Model Performance ---------\")\n",
    "print(\"Case 1:\")\n",
    "print(\"Model-a (Most Correlated): R2 =\", r2_a, \"MSE =\", mse_a)\n",
    "print(\"Model-b (Least Correlated): R2 =\", r2_b, \"MSE =\", mse_b)\n",
    "print(\"Case 2:\")\n",
    "print(\"Model-c (Most Correlated): R2 =\", r2_c, \"MSE =\", mse_c)\n",
    "print(\"Model-d (Least Correlated): R2 =\", r2_d, \"MSE =\", mse_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fceabf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda5fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DASzPUATlA_Z",
   "metadata": {
    "id": "DASzPUATlA_Z",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlations = data.corr()\n",
    "rating_correlations = correlations['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a2c72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bafd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load your dataset (same as before)\n",
    "df = pd.read_csv(\"C:/Users/Wassim/cleaned_ecommerce_dataset.csv\")\n",
    "\n",
    "# --- Sentiment Analysis ---\n",
    "def get_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()  # Create a sentiment analyzer\n",
    "    return analyzer.polarity_scores(text)['compound']  # Get sentiment score\n",
    "\n",
    "df['sentiment'] = df['review'].apply(get_sentiment)  # Add 'sentiment' column\n",
    "\n",
    "# --- Feature Selection ---\n",
    "X = df[['helpfulness', 'gender', 'category', 'sentiment']]  # Use 'sentiment'\n",
    "y = df['rating']\n",
    "\n",
    "# --- Model Training and Analysis (Your existing code) ---\n",
    "# ... (Train your models with the new 'sentiment' feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695910b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two most correlated features\n",
    "most_correlated_features = rating_correlations.nlargest(2).index.tolist()\n",
    "\n",
    "# Two least correlated features\n",
    "least_correlated_features = rating_correlations.nsmallest(2).index.tolist()  \n",
    "\n",
    "print(\"Most correlated features:\", most_correlated_features)\n",
    "print(\"Least correlated features:\", least_correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279740f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression  # Example regression model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load your dataset (assuming you have it in a DataFrame named 'data')\n",
    "\n",
    "# ... (Your code to load the dataset) ... \n",
    "\n",
    "# Assuming the following (replace with the actual output from your previous step):\n",
    "most_correlated_features = ['helpful', 'gender'] \n",
    "least_correlated_features = ['category', 'review'] \n",
    "\n",
    "# Split into features (X) and target variable (y)\n",
    "X = data[['helpful', 'gender', 'category', 'review']] \n",
    "\n",
    "y = data['rating']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "# Model 1: Most correlated features\n",
    "model_1 = LinearRegression()\n",
    "model_1.fit(X_train[most_correlated_features], y_train)\n",
    "y_pred_1 = model_1.predict(X_test[most_correlated_features])\n",
    "\n",
    "# Model 2: Least correlated features\n",
    "model_2 = LinearRegression()\n",
    "model_2.fit(X_train[least_correlated_features], y_train)\n",
    "y_pred_2 = model_2.predict(X_test[least_correlated_features])\n",
    "\n",
    "# Evaluation\n",
    "print('Model 1 (Most Correlated) R-squared:', r2_score(y_test, y_pred_1))\n",
    "print('Model 2 (Least Correlated) R-squared:', r2_score(y_test, y_pred_2))\n",
    "\n",
    "# You can also calculate other metrics like mean squared error (MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa26968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Step 1: Loading the Dataset\n",
    "data = pd.read_csv('C:/Users/Wassim/Downloads/cleaned_ecommerce_dataset.csv')\n",
    "\n",
    "# Step 2: Verify Data and Columns\n",
    "print(data.head())  #  Inspect the first few rows\n",
    "print(data.info())  #  Check data types and columns\n",
    "print(data.shape)   #  Print the dataset's dimensions\n",
    "\n",
    "# Step 3: Analyzing Correlations\n",
    "correlations = data.corr()\n",
    "rating_correlations = correlations['rating']\n",
    "\n",
    "most_correlated_features = rating_correlations.nlargest(2).index.tolist()\n",
    "least_correlated_features = rating_correlations.nsmallest(2).index.tolist()\n",
    "\n",
    "print(\"Most correlated features:\", most_correlated_features)\n",
    "print(\"Least correlated features:\", least_correlated_features)\n",
    "\n",
    "# Step 4: Preparing Data \n",
    "X = data[['helpfulness', 'gender', 'category', 'review']]  # Adjust columns if needed\n",
    "y = data['rating']\n",
    "\n",
    "# Step 5: Splitting Data (Make sure you have enough data first)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "# Step 6: Model Building and Evaluation\n",
    "def train_and_evaluate(features):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[features], y_train)\n",
    "    y_pred = model.predict(X_test[features])\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f'Model using features: {features}')\n",
    "    print('R-squared:', r2)\n",
    "    print('Mean Squared Error:', mse)\n",
    "    print('-------------------')\n",
    "\n",
    "train_and_evaluate(most_correlated_features)\n",
    "train_and_evaluate(least_correlated_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KATSn7hYlA_Z",
   "metadata": {
    "id": "KATSn7hYlA_Z"
   },
   "source": [
    "### Evaluate Models\n",
    "* Evaluate the performance of the four models with two metrics, including MSE and Root MSE\n",
    "* Print the results of the four models regarding the two metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fU8GPS9lA_Z",
   "metadata": {
    "id": "4fU8GPS9lA_Z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from textblob import TextBlob\n",
    "import numpy as np \n",
    "\n",
    "# ... (Your data loading, preprocessing, sentiment analysis code) ...\n",
    "\n",
    "# ---------------------- Step 4: Model Building ----------------------\n",
    "# ... (Your dataset creation code) ...\n",
    "\n",
    "models = {}\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    # ... (Your train_test_split code) ...\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    models[dataset_name] = {\n",
    "        'model': model,\n",
    "        'r2': r2_score(y_test, y_pred),\n",
    "        'mse': mean_squared_error(y_test, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),  # Calculate RMSE\n",
    "        'mae': mean_absolute_error(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# ---------------------- Step 5: Evaluation ----------------------\n",
    "print(\"Model Performance:\")\n",
    "for dataset_name, metrics in models.items():\n",
    "    print(f\"\\nModel {dataset_name}:\")\n",
    "    print(f\"  R-squared: {metrics['r2']}\")\n",
    "    print(f\"  MSE: {metrics['mse']}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']}\") \n",
    "    print(f\"  MAE: {metrics['mae']}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y9jx-eY6lA_a",
   "metadata": {
    "id": "Y9jx-eY6lA_a"
   },
   "source": [
    "### Visualize, Compare and Analyze the Results\n",
    "* Visulize the results, and perform ___insightful analysis___ on the obtained results. For better visualization, you may need to carefully set the scale for the y-axis.\n",
    "* Normally, the model trained with most correlated features and more training data will get better results. Do you obtain the similar observations? If not, please ___explain the possible reasons___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d42a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:/Users/Wassim/Downloads/cleaned_ecommerce_dataset.csv')\n",
    "\n",
    "# Potential Preprocessing\n",
    "df.dropna(subset=['helpfulness', 'rating'], inplace=True)  # Drop rows with missing values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85591e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "\n",
    "# Identify most/least correlated features with the 'rating' column\n",
    "most_corr_features = corr_matrix['rating'].nlargest(3)[1:3].index.tolist() \n",
    "least_corr_features = corr_matrix['rating'].nsmallest(3)[:-1].index.tolist() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3TNAIGDilA_a",
   "metadata": {
    "id": "3TNAIGDilA_a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = [[1, 2], [3, 4], [5, 6]]  # Sample data\n",
    "y = [0, 1, 1]  # Sample target labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit a model (replace with your actual model)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee01ac",
   "metadata": {
    "id": "f9ee01ac"
   },
   "source": [
    "### Data Science Ethics\n",
    "*Please read the following examples [Click here to read the example_1.](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) [Click here to read the example_2.](https://viborc.com/ethics-and-ethical-data-visualization-a-complete-guide/)\n",
    "\n",
    "*Then view the picture ![My Image](figure_portfolio2.png \"This is my image\")\n",
    "Please compose an analysis of 100-200 words that evaluates potential ethical concerns associated with the infographic, detailing the reasons behind these issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f30e9b",
   "metadata": {
    "id": "44f30e9b"
   },
   "outputs": [],
   "source": [
    "Privacy:  How was the athlete data collected?  Without explicitly mentioning data collection methods, concerns linger about whether athletes provided informed consent regarding the use of their performance metrics.  Respecting athlete privacy is paramount, especially when data can be linked to individuals.\n",
    "\n",
    "Bias:  The infographic lacks information about its data source. If the source itself is inherently biased (e.g., favoring certain nations or sports),  the presented medal counts might not offer an accurate or fair global representation.  Transparency about data origins is crucial for preventing misinterpretation.\n",
    "\n",
    "Potential for Misuse:  Focusing purely on medal counts can be misleading.  Factors beyond athletic prowess—like a country's investment in sports infrastructure or focus on specific disciplines—can heavily influence the results. Using this data to fuel nationalistic narratives or make unfair comparisons would be an ethical misuse.\n",
    "\n",
    "Responsible Data Science:  This infographic highlights the need for responsible data science practices.  Ethical considerations should encompass informed consent, addressing biases, anticipating potential misuse, and prioritizing transparency of data sources and methodologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511889d",
   "metadata": {
    "id": "0511889d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8610d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a37fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e468052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295faa3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
